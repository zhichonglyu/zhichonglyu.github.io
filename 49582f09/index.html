<!DOCTYPE html><html lang="default(en)" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>[object Object] | Zhichong Lyu (吕志冲)</title><meta name="author" content="Zhichong Lyu,lyuzhichong@my.swjtu.edu.cn"><meta name="copyright" content="Zhichong Lyu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="article"><meta property="og:title" content="[object Object]"><meta property="og:url" content="https://zhichonglyu.com/49582f09/index.html"><meta property="og:site_name" content="Zhichong Lyu (吕志冲)"><meta property="og:locale"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230036336.png"><meta property="article:published_time" content="2024-05-06T00:00:00.000Z"><meta property="article:modified_time" content="2025-02-04T17:54:18.091Z"><meta property="article:author" content="Zhichong Lyu"><meta property="article:tag" content="JFE"><meta property="article:tag" content="Asset pricing"><meta property="article:tag" content="Multimodal"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230036336.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202410021722692.png"><link rel="canonical" href="https://zhichonglyu.com/49582f09/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//www.clarity.ms"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?dd184d6ef82412aa4fd7bdce6dca0f79",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SD1RRPC8Q7"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SD1RRPC8Q7")</script><script>!function(t,e,n,a,c,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(c=e.createElement(a)).async=1,c.src="https://www.clarity.ms/tag/krja12ql80",(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(c,r)}(window,document,"clarity","script")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy Successful",error:"Copy Error",noSupport:"Browser Not Supported"},relativeDate:{homepage:!1,post:!1},runtime:"days",dateSuffix:{just:"Just now",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:{limitCount:30,languages:{author:"Author: Zhichong Lyu",link:"Link: ",source:"Source: Zhichong Lyu (吕志冲)",info:"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js",buttonText:"Load More"},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"[object Object]",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-02-05 01:54:18"}</script><script>(e=>{e.saveToLocal={set:(e,t,a)=>{var o;0!==a&&(o=Date.now(),localStorage.setItem(e,JSON.stringify({value:t,expiry:o+864e5*a})))},get:e=>{var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!(Date.now()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=(o,n={})=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},Object.keys(n).forEach(e=>{a.setAttribute(e,n[e])}),document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404291749220.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-solid fa-user-tie"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230036336.png)"><nav id="nav"><span id="blog-info"><a href="/" title="Zhichong Lyu (吕志冲)"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-solid fa-user-tie"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">[object Object]</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-05-06T00:00:00.000Z" title="Created 2024-05-06 08:00:00">2024-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-04T17:54:18.091Z" title="Updated 2025-02-05 01:54:18">2025-02-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Journal/">Journal</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">13k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>47mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="[object Object]"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230016443.png" alt=""></p><p><strong>封面来源：</strong>ChatGPT-4 DALL·E</p><p><strong>摘要：</strong>通过将机器学习应用于基于情绪的准确且经济高效的照片分类，作者引入了从大量新闻照片样本中获得的日度市场级投资者情绪指数（照片悲观度）。与行为模型一致，照片悲观情绪可以预测市场回报逆转和交易量。在恐慌情绪高涨期间，这种关系在套利限制高的股票中最为强烈。此外，本文研究了照片悲观度和新闻文本中蕴含的悲观主义在预测股票收益方面是互为补充还是相互替代，并发现了两者相互替代的证据。</p><p><strong>引用：</strong>Obaid, K., &amp; Pukthuanthong, K. (2022). A picture is worth a thousand words: Measuring investor sentiment by combining machine learning and photos from news. <em>Journal of Financial Economics</em>, 144(1), 273-297.</p><p><strong>文献：</strong><a target="_blank" rel="noopener" href="https://github.com/zhichonglyu/blogpdf/blob/main/49582f09.pdf"><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202402010500965.png" alt="PDF" style="display:inline;vertical-align:middle;zoom:33%"></a></p><p><strong>Keywords:</strong> Investor sentiment; Behavioral finance; Return predictability; Machine learning; Deep learning; Big data.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><blockquote><p>A good sketch is better than a long speech.</p><p>​ —— Napoleon Bonaparte</p></blockquote><h2 id="投资者情绪"><a href="#投资者情绪" class="headerlink" title="投资者情绪"></a><strong>投资者情绪</strong></h2><p>投资者情绪理解及预测市场收益（<strong><em>天气与股市收益</em></strong><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1540-6261.00556">$\color{blue}{Hirshleifer\ and\ Shumway,\ 2003}$</a>; <strong><em>运动情绪与股市收益</em></strong><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01262.x">$\color{blue}{Edmans\ et\ al.,\ 2007JF}$</a>; <strong><em>媒体情绪</em></strong><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock,\ 2007JF}$</a>; <strong><em>实证模型</em></strong><a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article-abstract/21/4/1453/1568740">$\color{blue}{Spiegel,\ 2008}$</a>）和横截面股票收益（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2006.00885.x">$\color{blue}{Baker\ and\ Wurgler,\ 2006}$</a>; <a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12612">$\color{blue}{Kozak\ et\ al.,\ 2018}$</a>）</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a><strong>贡献</strong></h2><p>（1）展示视觉内容在预测市场收益方面的重要性：</p><ul><li>构建日度投资者情绪指数，照片悲观度（<em>PhotoPes</em>），计算为在给定日期预测为负面的新闻照片的比例；</li><li><em>PhotoPes</em>对第二天的市场回报呈负相关，而与剩余交易周的市场回报率呈正相关；</li><li>反转凸显了指标对收益的非信息影响；</li><li>与投资者情绪代理一致，<em>PhotoPes</em>预测次日交易量增加，并更好地预测套利限制高股票的收益。</li></ul><p>（2）展示如何通过机器学习技术进行大规模照片分类，克服研究金融市场中视觉内容重要性的关键障碍：</p><ul><li><p><u>由于现代技术和对快速信息的需求，图片新闻越来越受欢迎</u>（<em>Photojournalism has increased in popularity due to modern technology and the demand for quick information.</em> ）</p></li><li><p>已有研究表明，照片可能比文字更有效地传达情感信息，有鉴于此，作者认为研究从新闻媒体照片中提取的情感与市场活动的关系非常重要（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1023/A:1024728626505">$\color{blue}{Chemtob\ et\ al.,\ 1999JTS}$</a>）；</p><ul><li>然而，由于分析照片的复杂性和人工筛选大量照片的成本，类似研究成本高昂、容易出错且繁琐；</li><li>依靠<strong>调查或众包网站</strong>（如 Amazon Mechanical Turk、MTurk）评估照片已成为<u>从照片中提取信息的主流方法</u>；经济学家对调查数据也持谨慎态度，因为其主观性过强，无法通过 ”客观的外部测量“验证（<a target="_blank" rel="noopener" href="https://www.journals.uchicago.edu/doi/abs/10.1086/ma.18.3585252">$\color{blue}{Vissing-Jorgensen,\ 2003}$</a>）；另外，<a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=99a37f7b5f2b4aa5857efe531fc75ce3fe340ab7">$\color{blue}{Singer\ (2002)}$</a>指出，当问题涉及敏感话题且答案取决于受访者的主观意愿时，受访者往往不太愿意认真如实回答问题；</li><li>使用<strong>机器学习方法</strong>可以减轻这种担忧（具体而言，作者应用卷积神经网络（CNN）这一常用于照片分类的机器学习技术，对大量新闻照片样本进行了基于情感的准确、可验证且低成本的分类）；</li></ul></li></ul><p>（3）比较了<em>PhotoPes</em>和新闻文本中嵌入的悲观情绪的预测能力：</p><ul><li>实证发现，<strong><u>新闻照片中的悲观情绪和新闻文字中的悲观情绪可以相互替代</u></strong>；</li><li><strong>照片突出时期，照片能从文字中吸引注意力</strong>（在新闻照片绝大多数是负面或正面的时期，新闻照片中的悲观情绪占主导地位。与此相反，当新闻照片是中性或混合性时，文字中蕴含的悲观情绪会占据主导地位）；</li><li><u>未有文献从新闻图片中捕捉有关投资者信念的有用和新颖信息的可能性，以及这些信息如何与文本中的信息相互作用</u>；</li></ul><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><strong>机器学习</strong></h2><p>金融研究人员已接受机器学习（<a target="_blank" rel="noopener" href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">$\color{blue}{Mullainathan\ and\ Spiess,\ 2017JEP}$</a>）</p><ul><li><a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Manela\ and\ Moreira\ (2017JFE)}$</a>00268-3/sbref0047)使用机器学习从WSJ文本中构建新闻隐含波动率指数；</li><li><a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article-abstract/31/7/2693/4824924?redirectedFrom=fulltext">$\color{blue}{Buehlmaier\ and\ Whited\ (2018)}$</a>通过分析财务报告构建了财务约束的衡量标准，并表明衡量标准与获得资本和股票收益有关；</li><li>机器学习结合金融数据：<ul><li>预测风险溢价（<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Gu\ et\ al.,\ 2020}$</a>00268-3/sbref0037)）；</li><li>寻找真正的风险因素（<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Feng\ et\ al.,\ 2020}$</a>00268-3/sbref0029)）；</li></ul></li></ul><p>机器学习新技术引入→→→分析大量照片可行→→→引入CNN从大量新闻照片样本中构建投资者情绪指数</p><p>预训练的 Google Inception (v3) 模型</p><ul><li>该模型未经过专门训练识别情绪，但包含大量有关图像的领域知识；</li><li>迁移学习（向预训练模型提供由情绪专门标记的额外训练照片样本，并将<u>原始模型的最终全连接层替换为仅含两类感兴趣的新层：消极和积极情绪</u>）；</li><li>该模型考虑了照片的多方面：objects、colors、facial expressions，以做出预测；</li><li>模型训练后，测试集样本准确率为87.1%；</li><li>使用该模型对WSJ照片进行投资者情绪预测→→→使用情绪预测构建日度投资者情绪指数（照片悲观度，PhotoPes）→→→给定日期负面照片所占比例；</li></ul><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>1926-2018数据来源于Getty Images，<a target="_blank" rel="noopener" href="https://www.kuntara.net/">日度情绪数据</a>；使用WSJ数据的文献支撑：<a target="_blank" rel="noopener" href="https://www.degruyter.com/document/doi/10.1515/9781400865536">$\color{blue}{Shiller\ (2015)}$</a>、<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock\ (2007JF)}$</a>、<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Garcia\ (2013)}$</a>00268-3/sbref0032)；</p><h2 id="Finding"><a href="#Finding" class="headerlink" title="Finding"></a>Finding</h2><p><strong><em>PhotoPes</em>具有投资者情绪代理的特征</strong></p><ul><li>首先，检验<em>PhotoPes</em>与美国主要股指和交易所交易基金（ETF）的关系：<ul><li><a target="_blank" rel="noopener" href="https://www.journals.uchicago.edu/doi/abs/10.1086/261703">$\color{blue}{De\ Long\ et\ al.\ (1990JPE)}$</a>行为模型预计投资者情绪应当预测市场收益反转【当情绪高（低）时，非理性投资者将增加（减少）他们对资产的需求，从而推高（下跌）价格，偏离基本面。但由于套利的限制<sup><a href="#fn_ 1" id="reffn_ 1"> 1</a></sup>;<a target="_blank" rel="noopener" href="https://academic.oup.com/qje/article-abstract/111/4/1135/1932203">$\color{blue}{Pontiff\ (1996QJE)}$</a>、<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1997.tb03807.x">$\color{blue}{Shleifer\ and\ Vishny\ (1997JF)}$</a>】；然而，随时间推移，理性的投资者将利用错误定价，导致价格回到基本水平。</li><li><em>PhotoPes</em>与次日市场收益呈负相关，与剩余交易周的市场收益呈正相关。该反转凸显了本文衡量标准对收益的$\color{red}{非信息性影响}$；</li></ul></li><li>其次，<strong>照片悲观情绪与文字悲观情绪（TextPes）显著相关</strong><ul><li>表明这两个变量所捕捉的信息类型存在一些共性；</li><li>研究照片和文本中蕴含的悲观情绪如何相互作用后，发现新闻照片中的悲观情绪和新闻文本中的悲观情绪<strong>可以相互替代</strong>；</li><li>实证表明$\color{red}{在照片显著的时期，照片会从文字中捕获注意力}$。</li></ul></li><li>第三，深入了解新闻和金融市场背景下，哪类信息通过照片传播更有效：<ul><li>相较基线时期，恐惧情绪高涨时期照片中蕴含的悲观情绪的系数大两倍以上，而文本信息的系数则基本保持不变。即与文字相比，<strong>照片在传递创伤性新闻方面更为有效</strong>（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1023/A:1024728626505">$\color{blue}{Chemtob\ et\ al.,\ 1999JTS}$</a>）；</li><li>高度恐慌时期，照片对市场收益的影响最为显著；</li></ul></li><li>第四，进一步验证<em>PhotoPes</em>作为投资者情绪代理的有效性，表明其对难以套利的股票影响更大：<ul><li>基于套利成本高的股票对投资者情绪冲击最敏感的预测（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2006.00885.x">$\color{blue}{Baker\ and\ Wurgler,\ 2006}$</a>），本文构建基于firm idiosyncratic volatility和size的投资组合；</li><li>实证发现，<strong><em>PhotoPes</em>对idiosyncratic volatility最高和size最小投资组合的收益影响最大</strong>；</li></ul></li><li>第五，更好地理解<em>PhotoPes</em>与市场回报相关的渠道：<ul><li><em>PhotoPes</em>值的高低可预测次日异常交易量的增加，该证据进一步验证PhotoPes能够捕捉投资者情绪（<a target="_blank" rel="noopener" href="https://www.journals.uchicago.edu/doi/abs/10.1086/261703">$\color{blue}{De\ Long\ et\ al.,\ 1990JPE}$</a>）；</li></ul></li><li>最后，稳健性检验表明，即使在控制了极端收益之后，主要结果对不同的可变结构标准仍然可靠，<em>PhotoPes</em>生成了显著样本外$R^2$；</li></ul><blockquote id="fn_ 1"><sup>1</sup>. 有限套利或称套利限制是指由于风险、成本、信息不对称和制度性约束等因素的影响，使得套利者的套利行为受到限制。<a href="#reffn_ 1" title="Jump back to footnote [ 1] in the text."> &#8617;</a></blockquote><h2 id="文献扩展"><a href="#文献扩展" class="headerlink" title="文献扩展"></a>文献扩展</h2><h3 id="与投资者情绪文献相关"><a href="#与投资者情绪文献相关" class="headerlink" title="与投资者情绪文献相关"></a>与投资者情绪文献相关</h3><ul><li>鉴于投资者情绪的多维性，研究人员一直在寻找不同的方法来衡量投资者情绪（<a target="_blank" rel="noopener" href="https://www.annualreviews.org/doi/abs/10.1146/annurev-financial-110217-022725">$\color{blue}{Zhou,\ 2018ARFE}$</a>）；</li><li>使用新闻（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock,\ 2007JF}$</a>）、谷歌搜索数据（<a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article/28/1/1/1682440">$\color{blue}{Da\ et\ al.,\ 2015RFS}$</a>）、推特数据（<a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article-abstract/27/5/1367/1581938">$\color{blue}{Chen\ et\ al.,\ 2014RFS}$</a>）、公司财务报告（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2010.01625.x">$\color{blue}{Loughran\ and\ McDonald,\ 2011JF}$</a>；<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0304405X18302770">$\color{blue}{Jiang\ et\ al.,\ 2019JFE}$</a>）、天气（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1540-6261.00556">$\color{blue}{Hirshleifer\ and\ Shumway,\ 2003JF}$</a>）和体育赛事（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01262.x">$\color{blue}{Edmans\ et\ al.,\ 2007JF}$</a>）代表投资者情绪（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/0022-1082.00379">$\color{blue}{Hirshleifer,\ 2001JF}$</a>）；</li></ul><h3 id="扩展投资者情绪和新闻文献"><a href="#扩展投资者情绪和新闻文献" class="headerlink" title="扩展投资者情绪和新闻文献"></a>扩展投资者情绪和新闻文献</h3><ul><li>新闻是投资者信念的可信替代物，因为新闻界有需求方动机来迎合读者的信念（<a target="_blank" rel="noopener" href="https://www.degruyter.com/document/doi/10.1515/9781400865536">$\color{blue}{Shiller,\ 2005IE}$</a>）。</li><li><a target="_blank" rel="noopener" href="https://www.aeaweb.org/articles?id=10.1257/0002828054825619">$\color{blue}{Mullainathan\ and\ Shleifer\ (2005AER)}$</a>总结了传播学、心理学、记忆和信息处理方面的文献，这些文献支持人们从与其信念一致的内容中获得效用的观点;</li><li><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA7195">$\color{blue}{Gentzkow\ and\ Shapiro\ (2010Econ)}$</a>为这一理论提供了实证证据，具体而言媒体的倾向性在很大程度上归因于消费者的偏好；</li><li><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock\ (2007JF)}$</a>的<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Garcia\ (2013JF)}$</a>00268-3/sbref0032)研究表明，新闻文本中蕴含的情绪可以预测市场收益和交易量；</li><li>本文通过揭示新闻照片中包含与金融市场相关的内容，进一步扩展文献；</li></ul><h3 id="将视觉刺激心理学文献扩展到金融和经济学"><a href="#将视觉刺激心理学文献扩展到金融和经济学" class="headerlink" title="将视觉刺激心理学文献扩展到金融和经济学"></a>将视觉刺激心理学文献扩展到金融和经济学</h3><p><strong>关于新闻中视觉刺激的证据不一</strong></p><ul><li><p>一方面，<strong>图片优势效应</strong>（$\color{green}{picture\ \ superiority\ \ effect}$）是心理学中的一个已知概念，是指<u>图片比文字更容易被回忆起</u>的现象（<a target="_blank" rel="noopener" href="https://psycnet.apa.org/journals/xlm/2/5/523/">$\color{blue}{Nelson\ et\ al.,\ 1976JEPHLM}$</a>；<a target="_blank" rel="noopener" href="https://psycnet.apa.org/record/1991-98882-000">$\color{blue}{Paivio,\ 1991Book}$</a>）;</p><ul><li><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.1992.tb00776.x">$\color{blue}{Newhagen\ and\ Reeves\ (1992JoC)}$</a>将图片优势效应延伸到了新闻媒体，发现新闻中的视觉内容比新闻中的文字内容更容易被回忆；</li><li>此外，<a target="_blank" rel="noopener" href="https://www.biblio.com/book/eyes-news-pegie-starkmario-r-garcia/d/1435923367">$\color{blue}{Garcia\ and\ Stark\ (1991Book)}$</a>以及<a target="_blank" rel="noopener" href="https://academic.oup.com/joc/article-abstract/65/6/997/4082343">$\color{blue}{Powell\ et\ al.\ (2015JoC)}$</a>发现，图片在新闻中起到吸引注意力的作用；</li></ul></li><li><p>另一方面，研究人员发现，当信息简单易懂时，视觉内容最为有效，而当信息复杂时，文字内容最为有效（<a target="_blank" rel="noopener" href="https://psycnet.apa.org/record/1977-21082-001">$\color{blue}{Chaiken\ and\ Eagly,\ 1976JPSP}$</a>）；</p></li><li>本文以金融决策为背景，对以上两方面文献进行调和；</li></ul><h3 id="扩展关于视觉内容预测金融市场"><a href="#扩展关于视觉内容预测金融市场" class="headerlink" title="扩展关于视觉内容预测金融市场"></a>扩展关于视觉内容预测金融市场</h3><ul><li>一些研究记录了一张照片如何能够预测重要结果，如政治选举、个人贷款决策、公司市值和首席执行官薪酬（ <a target="_blank" rel="noopener" href="https://www.science.org/doi/abs/10.1126/science.1110589">$\color{blue}{Todorov\ et\ al.,\ 2005Science}$</a>；<a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article-abstract/25/8/2455/1570804">$\color{blue}{Duarte \ et\ al.,\ 2012RFS}$</a>；<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/fire.12234">$\color{blue}{Halford\ and\ Hsu,\ 2020FR}$</a>；<a target="_blank" rel="noopener" href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2016.2484">$\color{blue}{Graham\ et\ al.,\ 2017MS}$</a>）；</li></ul><ul><li><a target="_blank" rel="noopener" href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2020.3747">$\color{blue}{Bazley\ et\ al.\ (2021MS)}$</a>记录了用红色显示财务信息如何降低投资者的风险偏好和乐观情绪；</li><li><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1475-679X.12164">$\color{blue}{Blankespoor\ et\ al.\ (2017JAR)}$</a>展示了视频演示中对管理层的看法与公司价值的关系；</li><li>本文首次利用机器学习从新闻照片中开发出投资者情绪代理预测股市收益；</li></ul><h1 id="Data-1"><a href="#Data-1" class="headerlink" title="Data"></a>Data</h1><h2 id="Photo-classification"><a href="#Photo-classification" class="headerlink" title="Photo classification"></a>Photo classification</h2><h3 id="机器学习照片分类模型的主要任务是能够在最少人工参与的情况下识别照片内容"><a href="#机器学习照片分类模型的主要任务是能够在最少人工参与的情况下识别照片内容" class="headerlink" title="机器学习照片分类模型的主要任务是能够在最少人工参与的情况下识别照片内容"></a>机器学习照片分类模型的主要任务是能够在最少人工参与的情况下识别照片内容</h3><ul><li>CNN 是一种深度神经网络，可用于照片分类（<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">$\color{blue}{Krizhevsky\ et\ al.,\ 2012ANIPS}$</a>），最近的研究利用CNN照片分类模型：<ul><li>通过分析卫星图像识别太阳能电池板的安装（<a target="_blank" rel="noopener" href="https://www.cell.com/joule/pdf/S2542-4351(18">$\color{blue}{Yu\ et\ al.,\ 2019Jonle}$</a>30570-1.pdf)）、奴隶营的位置（<a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.363.6429.804">$\color{blue}{Scoles,\ 2019Science}$</a>）以及欠发达国家的贫困程度（<a target="_blank" rel="noopener" href="https://www.science.org/doi/abs/10.1126/science.aaf7894">$\color{blue}{Jean\ et\ al.,\ 2016Science}$</a>）；</li><li>与本文类似，<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/9179">$\color{blue}{You\ et\ al.\ (2015PAAAICAI)}$</a>也采用 CNN 进行图像情感分析，并在社交媒体的照片上取得了很高的准确率；</li></ul></li></ul><h3 id="构建能够预测照片可能引发投资者情绪的模型"><a href="#构建能够预测照片可能引发投资者情绪的模型" class="headerlink" title="构建能够预测照片可能引发投资者情绪的模型"></a>构建能够预测照片可能引发投资者情绪的模型</h3><ul><li><p>基于Google Inception (v3) (<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html">$\color{blue}{Szegedy\ et\ al.,\ 2016PIEEECCVPR)}$</a>建立一个照片分类模型</p><ul><li>Google Inception (v3) 是一个CNN模型，在ImageNet学术竞赛ILSVRC（ImageNet Large-Scale Visual Recognition Challenge，ImageNet大规模视觉识别挑战赛）中，它在1000个类别的照片分类方面表现出色，并被广泛应用于实践和研究中<sup><a href="#fn_ 2" id="reffn_ 2"> 2</a></sup><sup><a href="#fn_ 3" id="reffn_ 3"> 3</a></sup>；</li></ul></li><li><p>TensorFlow是谷歌大脑团队开发的一个开源软件库，是机器学习应用的常用工具，提供了一个预训练的谷歌 Inception (v3) 模型</p><ul><li>从预训练Google Inception (v3) 模型（在ImageNet数据集上训练）入手，利用迁移学习针对本文特定应用对模型进行<strong>微调</strong>；</li><li>预训练的Google Inception (v3) 有1000个不同的类别，其初始目的为：ILSVRC；</li><li>【<em>从头开始训练照片分类模型需要大量的训练样本，而且计算成本很高，133 1167 张预先标记的图片训练而成</em>】通过<strong>迁移学习</strong>，重新利用预训练模型中存储的领域知识，只需将<strong><u>最后的全连接层替换为具有所需类别数量的新层</u></strong>，并<u>用更少的训练样本重新训练最后全连接层的参数</u>，轻松构建出本文所需模型（<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10994-012-5310-y">$\color{blue}{Yang\ et\ al.,\ 2013ML}$</a>）；</li><li><strong>微调模型输出照片具有正面情感的概率和照片具有负面情感的概率</strong>；</li></ul></li></ul><blockquote id="fn_ 2"><sup>2</sup>. 在来自ImageNet数据集的验证集上，前五名和前一名的错误率分别为 3.5% 和 17.3%。前一错误率是指模型没有将正确类别作为其最高预测概率的时间百分比。前五名错误率是指该模型未将正确类别作为其概率排名前五的预测结果的时间百分比。<a href="#reffn_ 2" title="Jump back to footnote [ 2] in the text."> &#8617;</a></blockquote><blockquote id="fn_ 3"><sup>3</sup>. ImageNet 数据集可在以下网址免费下载： <a target="_blank" rel="noopener" href="http://image-net.org/download">http://image-net.org/download</a> 。<a href="#reffn_ 3" title="Jump back to footnote [ 3] in the text."> &#8617;</a></blockquote><h3 id="未开发更精细分类模型原因"><a href="#未开发更精细分类模型原因" class="headerlink" title="未开发更精细分类模型原因"></a>未开发更精细分类模型原因</h3><ul><li>首先，<strong>更精细的分类需要主观判断</strong>。大多数文本情感分析，如<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2010.01625.x">$\color{blue}{Loughran\ and\ McDonald\ (2011JF)}$</a>的分析，都是对词语进行二元分类（如正面词语和负面词语） <sup><a href="#fn_ 4" id="reffn_ 4">4</a></sup>。</li><li>其次，本文尝试创建一个具有更精细类别的照片分类模型，但模型无法收敛，<u>因为<strong>其无法检测出不同细分类之间的明显区别</strong>。</u></li></ul><blockquote id="fn_ 4"><sup>4</sup>. 例如，“疾病”和“崩溃”显然是负面的，但其负面程度却不太明显。语境和个人对单词本身的主观解释也会影响负面情绪的程度。<a href="#reffn_ 4" title="Jump back to footnote [ 4] in the text."> &#8617;</a></blockquote><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><ul><li><strong>迁移学习需要一个由情感标签照片组成的训练集。</strong></li><li>本文使用DeepSent数据集进行训练<sup><a href="#fn_ 5" id="reffn_ 5"> 5</a></sup>， 由<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/9179">$\color{blue}{You\ et\ al.\ (2015PAAAICAI)}$</a>收集并按情感标记的照片集。</li><li>使用DeepSent数据集的主要优势在于，情感标签是通过<strong>众包网站（如MTurk）</strong> <strong><u>验证</u></strong>的，以确保标签的正确性<sup><a href="#fn_ 6" id="reffn_ 6"> 6</a></sup>。【<em>为了提高可靠性，作者使用MTurk调查的所有五位参与者都同意照片情感的照片。这一限制将训练样本减少到 882 张照片。</em>】</li><li>该模型使用0.01的学习率和500个学习步骤进行训练<sup><a href="#fn_ 7" id="reffn_ 7"> 7</a></sup>。将训练批次大小设置为100张照片，保留10%的训练照片作为验证样本，20%的训练照片作为测试样本（照片随机分配到各集合）<sup><a href="#fn_ 8" id="reffn_ 8"> 8</a></sup>。</li><li>训练集是训练过程中用于调整最终全连接层权重的照片集。验证集是一组照片，不用于调整最后一个完全连接层的权重，但它们的唯一目的是通过验证训练准确性的任何提高不会以牺牲超出的代价来帮助最大限度地减少过度拟合。样本性能。最后，测试集是模型在训练过程中从未见过的一组照片，用于计算模型的最终准确度得分。除了使用验证集来限制过度拟合之外，我们还通过使用增强技术（例如翻转、缩放）并在模型中添加正则化技术（例如 dropout 和标签平滑）来扩大训练集（Szegedy 等人，2016）。</li><li>训练集是在训练过程中用于调整最后全连接层权重的照片集。<strong>验证集是不用于调整最后全连接层权重的照片集，但其唯一目的是通过验证训练精度的提高是否以牺牲样本外性能为代价，从而帮助最大限度地减少过拟合。</strong>最后，测试集是模型在训练过程中从未见过的一组照片，用于计算模型的最终准确率得分。除了使用验证集来限制过拟合外，作者还通过使用增强技术（e.g., flip, scale）以及在模型中添加正则化技术（如dropout and label smoothing ）来扩大训练集（<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html">$\color{blue}{Szegedy\ et\ al.,\ 2016PIEEECCVPR}$</a>）。</li></ul><blockquote id="fn_ 5"><sup>5</sup>. Deep Sent 包含 1269 张带标签的照片，可供下载：<a target="_blank" rel="noopener" href="https://qzyou.github.io/#datasets。">https://qzyou.github.io/#datasets。</a><a href="#reffn_ 5" title="Jump back to footnote [ 5] in the text."> &#8617;</a></blockquote><blockquote id="fn_ 6"><sup>6</sup>. 验证标签的过程是一项昂贵且耗时的任务。然而，不验证标签可能会导致模型性能不佳，因为训练数据可能有噪音，因此我们采取额外的步骤来确认标签。<a href="#reffn_ 6" title="Jump back to footnote [ 6] in the text."> &#8617;</a></blockquote><blockquote id="fn_ 7"><sup>7</sup>. 学习率是模型中的权重在每个学习步骤之后可以改变的量。学习步骤是指我们通过模型传递训练集的时间。这些是 CNN 图像分类应用中使用的常见值（<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/9179">$\color{blue}{You\ et\ al.,\ 2015PAAAICAI}$</a>）。<a href="#reffn_ 7" title="Jump back to footnote [ 7] in the text."> &#8617;</a></blockquote><blockquote id="fn_ 8"><sup>8</sup>. 作者避免使用大部分训练样本作为测试集，以确保训练集中有足够的照片。文献中存在一些差异，但大多数研究将大部分样本分配给训练集（通常在70%到80%之间），其余样本分配给验证集和测试集。例如，<a target="_blank" rel="noopener" href="https://www.cell.com/joule/pdf/S2542-4351(18">$\color{blue}{Yu\ et\ al.\ (2019Jonle)}$</a>30570-1.pdf)使用77/3/20分割。<a href="#reffn_ 8" title="Jump back to footnote [ 8] in the text."> &#8617;</a></blockquote><h3 id="模型检验（测试集）"><a href="#模型检验（测试集）" class="headerlink" title="模型检验（测试集）"></a>模型检验（测试集）</h3><p>$\color{blue}{图1}$绘制该模型在训练步骤中的训练和验证准确率。测试准确率达到87.1%<sup><a href="#fn_ 9" id="reffn_ 9"> 9</a></sup>，该值与文献中其他照片情感分类模型的准确率相似。例如，<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0262885617300355">$\color{blue}{Campos\ et\ al.\ (2017)}$</a>比较了在 DeepSent上训练的CNN模型的各种修改，并报告了测试集准确率值，其范围在78.3%到83.0%之间。为了更好地测试本文模型性能，作者还计算了召回率（86.2%）、精确率（94.3%）和 F1（90.1%）<sup><a href="#fn_ 10" id="reffn_ 10"> 10</a></sup>。</p><p>（<em>精确率衡量了模型在所有预测为阳性的照片中识别阳性照片的准确度（当假阳性的成本很高时，这是一个重要指标）。Recall 衡量模型从样本中所有阳性照片中识别出阳性照片的准确度（当假阴性的代价较高时，这是一个重要指标）。F1 是精确度和召回率的调和平均值。</em>）</p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404270530784.png" alt=""></p><blockquote id="fn_ 9"><sup>9</sup>. 我们避免使用大部分训练样本作为测试集，以确保训练集中有足够的照片。文献中存在一些差异，但大多数研究将大部分样本分配给训练集（通常在 70% 到 80% 之间），其余样本分配给验证集和测试集。例如，$Accuracy = \text{(True Positive + True Negative) / + (True Positive + True Negative + False Positive + False Negative)}$<a href="#reffn_ 9" title="Jump back to footnote [ 9] in the text."> &#8617;</a></blockquote><blockquote id="fn_ 10"><sup>10</sup>. $Recall = True positive / (True Positive + Falese Positive)$，$Precision = True Positive / (True Positive + False Positive)$，$F_1 = 2 <em>Recall </em>Precision / (Recall + Precision)$<a href="#reffn_ 10" title="Jump back to footnote [ 10] in the text."> &#8617;</a></blockquote><h3 id="模型检验（人工标注WSJ数据）"><a href="#模型检验（人工标注WSJ数据）" class="headerlink" title="模型检验（人工标注WSJ数据）"></a>模型检验（人工标注WSJ数据）</h3><p><strong>问题：</strong>DeepSent训练集中的照片来自社交媒体，可能与 WSJ 样本中的专业新闻照片类型不太相似，使用 DeepSent训练集训练的模型可能无法对专业照片进行准确分类。</p><p><strong>解决：</strong>从WSJ样本中随机选取100张照片，并对每张照片进行分类（分类工作由五个人在MTurk中预先完成）<sup><a href="#fn_ 11" id="reffn_ 11"> 11</a></sup>。通过模型对照片进行预测，并将预测结果与从MTurk收集到的回复进行比较。作者将分析结果总结为以下混淆矩阵<sup><a href="#fn_ 12" id="reffn_ 12"> 12</a></sup>：</p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281425821.png" alt=""></p><p>根据上述混淆矩阵，计算模型在对WSJ的专业新闻照片样本进行分类时的表现。准确率为 76.0%；召回率为92.8%；精确率为77.1%；F1为84.2%。鉴于样本中存在不平衡的类别，因此关注F1至关重要。准确率、召回率、精确率和 F1 数值与<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/9179">$\color{blue}{You\ et\ al.\ (2015PAAAICAI) }$</a>基于使用DeepSent数据集训练的照片分类算法所报告的数值接近（他们的结果汇总见 <a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/9179">$\color{blue}{You\ et\ al.\ (2015PAAAICAI) }$</a>的表1）。总体而言，该表显示本文使用DeepSent训练集训练的模型在新闻照片分类方面表现良好。</p><blockquote id="fn_ 11"><sup>11</sup>. 要求 MTurk “员工”的 HIT 通过率超过 95%，并且必须位于美国。<a href="#reffn_ 11" title="Jump back to footnote [ 11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. 在机器学习领域，混淆矩阵是总结分类模型性能的常用方法。表中的数字代表测试样本中属于true positives, false positives, false negatives, and true negatives这四个类别的照片数量。<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><p>$\color{blue}{附录表A1}$列出了样本中被预测为具有最高负面（顶部）或正面（底部）情绪概率的前 20 张照片。$PhotoNeg_{it}$是第$i$张照片在第$t$天出现负面情绪的概率。总体而言，这些示例照片有助于确认照片分类模型的正确运行。</p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281451163.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281452941.png" alt=""></p><h2 id="The-Wall-Street-Journal-sample"><a href="#The-Wall-Street-Journal-sample" class="headerlink" title="The Wall Street Journal sample"></a>The Wall Street Journal sample</h2><p>WSJ关注重大事件，目标受众是对经济和金融市场特别感兴趣的读者。WSJ为订户提供在线访问过去文章的服务，最早可追溯到1997年12月。但是，2008年9月之前的文章很少附带照片。缺少2008年9月之前照片的原因之一是，《华尔街日报》并不拥有许多照片的版权，而是从Associated Press, Reuters, and Getty Images等其他新闻和媒体机构获得了照片的有限授权。一旦许可证到期，照片就会被删除。</p><p>从WSJ的2008年9月至2020年9月期间以下版面收集了每篇文章的标题和摘要、相关照片以及文章发表的时间戳：“Business,” “Economy,” “Markets,” “Politics,” and “Opinion”。只要是与经济相关的国际话题（如 “Asia Business”）均会收录，这些栏目报道与公司或行业相关的重大事件以及总体市场状况。</p><p><strong><u>共收集148823篇文章，时间跨度为3048个交易日。使用第2.1节中讨论的照片分类模型对这些照片进行情感分类。</u></strong></p><h2 id="Variable-construction"><a href="#Variable-construction" class="headerlink" title="Variable construction"></a>Variable construction</h2><h3 id="PhotoPes"><a href="#PhotoPes" class="headerlink" title="$PhotoPes$"></a>$PhotoPes$</h3><p>主要变量”$PhotoPes$”的计算方法为：预测在特定日期为负数的照片所占比例。第$t$天的$PhotoPes$计算公式为：</p><script type="math/tex;mode=display">PhotoPes_t=\frac{\sum_i(Neg_{it})}{n_t}</script><p>其中$Neg_{it}$是一个指标变量，表示第$t$天的照片$i$是否会产生负面情绪。分母$n_t$对应的是$t$日的照片数量。网站（<a target="_blank" rel="noopener" href="https://www.kuntara.net/）提供相关数据。">https://www.kuntara.net/）提供相关数据。</a></p><p>$PhotoPes$并非简单的二元衡量标准。虽然单张照片被分为负面或正面，但$PhotoPes$是一个连续的测量值。与负面照片较少的日子相比，负面照片较多的日子的$PhotoPes$值较高，这与文献中许多情感度量方法的理念相同。本文使用另一个版本的$PhotoPes$展示基线结果，该版本使用预测可能性 （$PhotoPes$）代替指标变量$Neg<em>{it}$。$\color{blue}{表 2\ Panel\ B}$显示，结果一致。如果将指标变量 ($Neg</em>{it}$) 的临界值从50%调整到 55%，结果一致<br>)的临界值从 50% 调整为 55%，我们的结果仍然成立（$\color{blue}{表A 2\ Panel\ A}$）。</p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281516190.png" alt=""></p><h3 id="TextPes"><a href="#TextPes" class="headerlink" title="$TextPes$"></a>$TextPes$</h3><p><u><strong>本文目标之一是比较照片和文字中蕴含的悲观情绪。</strong></u>受<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Manela\ and\ Moreira\ (2017JFE)}$</a>00268-3/sbref0047)以及<a target="_blank" rel="noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3307057">$\color{blue}{Cong\ et\ al.\ (2018SSRN)}$</a>的启发，作者分析了文章的标题和摘要。从概念上讲，用来识别照片中悲观情绪的机器学习方法包括特征之间的高度非线性关系。为了便于对照片和文本中的悲观情绪进行公平比较，<strong>避免使用词典方法对文本进行分类</strong>。相反，使用斯坦福大学<strong>CoreNLP软件中的情感工具来评估每个句子中的悲观情绪，并将文本中所有句子的平均悲观情绪得分作为文章的悲观情绪得分</strong>，即$TextNeg$（来源：<a target="_blank" rel="noopener" href="https://stanfordnlp.github.io/CoreNLP/）。">https://stanfordnlp.github.io/CoreNLP/）。</a></p><p>该情感工具以递归神经张量网络（RNTN）为基础，在包含215154个短语的数据集上进行训练，这些短语带有细粒度的情感标签（标度：{“ 负面 “ = 1; “ 中性 “ = 0.5; “ 正面 “ = 0}）。<em>该RNTN模型在短语方面表现尤为突出，短语准确率高达 85.4%</em>（<a target="_blank" rel="noopener" href="https://aclanthology.org/D13-1170.pdf">$\color{blue}{Socher\ et\ al.,\ 2013PCEMNLP}$</a>）。由于WSJ文章的标题和摘要部分通常都很简短，因此该工具非常适合。$TextPes$的计算公式如下：</p><script type="math/tex;mode=display">TextPes_t=\frac{\sum_i(TextNeg_{it})}{n_t},</script><p>其中$TextNeg_{it}$是CoreNLP模型中第$t$天每篇文章$i$的悲观分数。分母$n_t$对应于日期$t$中的文章数量。$PhotoPes$和 $TextPes$进行1%水平缩尾处理。结果在不进行缩尾处理的情况下保持不变（$\color{blue}{表A 2\ Panel\ B}$）。</p><p>基于RNTN方法而不是基于字典的方法，原因如下：</p><ul><li>首先，本文用来识别图像中情感的方法包含了特征之间的高度非线性关系，而<strong>对文本采用基本方法会让图像捕捉到更微妙的情感特征，从而使结果对作者有利，而对文本采用简单的方法则无法捕捉到这些情感特征</strong>。<em>例如，字典方法对“好”和“很棒”等积极词给予相同的权重，并且不考虑上下文和单词组合，而RNTN方法则考虑这一点，因此使用具有相当复杂性的方法提取情感合适。</em></li><li>其次，本文分析的是文章的标题和摘要部分，比全文短得多。<strong>字典方法最适合10-K或完整新闻稿等长文本，因为其避免任何单词与字典中的单词重叠的情况</strong>（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2010.01625.x">$\color{blue}{Loughran\ and\ McDonald,\ 2011JF}$</a>）。<em>本文数据中92.47%的文章标题和摘要部分不包含任何属于Loughran和McDonald词典的净负面或正面词汇。相比之下，使用机器学习方法，只有63.66%的标题和文章摘要部分包含中性情绪。</em></li></ul><h2 id="Descriptive-statistics"><a href="#Descriptive-statistics" class="headerlink" title="Descriptive statistics"></a>Descriptive statistics</h2><p>$\color{blue}{表1\ Panel\ A}$报告了悲观变量的汇总统计量。从2008年9月到2020年9月，有3048个交易日。平均而言，$PhotoPes$为 0.228，即22.8%的照片在某一天会出现负面情绪。平均而言，$TextPes$为0.686，表明平均而言，文章的标题和摘要文本由负面句子组成（Scale：{“Negative”= 1；”Neutral “= 0.5；”Positive”= 0}）。所有变量都有明显的一阶自相关性，因此具有持久性（在测试中解决了自相关性问题）。</p><p>$\color{blue}{表1\ Panel\ B}$报告主要美国股指和ETF的汇总统计数据。$\color{blue}{表1\ Panel\ C}$报告了$PhotoPes$与$TextPes$的关系。计算$PhotoPes$和$TextPes$之间的成对相关性和对应$p$值，$PhotoPes$和$TextPes$呈正相关（相关系数=0.079，$p$值 &lt; 0.01）。然而，相关性不是很高，这表明照片中存在的一些信息与WSJ文章标题和摘要部分中的文本不同。在文章层面，62.65%的文章文字和图片语气一致。$\color{blue}{图2}$显示$PhotoPes$和$TextPes$的每日和每月频率的时间序列，在恐慌情绪高涨的时期，包括2008年至2011年的次贷危机和2020年的COVID-19危机，这两个序列都会出现较大的峰值。例如，2011年两个变量的峰值特别大，可以归因于美国与基地组织之间的高度紧张，从美国政府于 2011年5月9日处决基地组织头目就可见一斑。此外，2011年3月11日日本东北地区发生地震和海啸后，美国西海岸也发出了海啸警报。 2011年3月16日，美国食品批发价格上涨3.9%，创1974年11月以来最大单月涨幅。有关通胀失控的传言只会让情况变得更糟。综上所述，重大新闻事件往往伴随着悲观照片，而这些照片正是我们的衡量指标。最后，与正常时期相比，<strong>市场动荡时期$PhotoPes$和$TextPes$更加接近</strong><sup><a href="#fn_ 13" id="reffn_ 13"> 13</a></sup>。</p><blockquote id="fn_ 13"><sup>13</sup>. 市场动荡期间，$PhotoPes$和$TextPes$之间的相关系数最高。相关性在2008年第四季度达到峰值，约为0.5，然后在 2009年1月下降至0.2。同样，在2020年第二季度，新冠疫情期间，相关性跃升至 0.36。<a href="#reffn_ 13" title="Jump back to footnote [ 13] in the text."> &#8617;</a></blockquote><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281441328.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404290941510.png" alt=""></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>行为模型通过做出两个假设打破理性投资者和市场效率模型：</p><ul><li>首先，行为模型考虑到一些投资者是非理性的并且能够影响价格（<a target="_blank" rel="noopener" href="https://www.journals.uchicago.edu/doi/abs/10.1086/261703">$\color{blue}{De\ Long\ et\ al.,\ 1990JPE}$</a>）。诸如推断（<a target="_blank" rel="noopener" href="https://psycnet.apa.org/record/1984-03110-001">$\color{blue}{Tversky\ and\ Kahneman,\ 1983}$</a>）和过度自信（<a target="_blank" rel="noopener" href="https://psycnet.apa.org/journals/xhp/3/4/552/">$\color{blue}{Fischhoff\ et\ al.,\ 1977}$</a>）等偏差可能会导致非理性投资者增加对金融资产的需求，从而推动价格超出经济基本面。</li><li>其次，套利的限制阻止理性投资者全面、立即纠正价格偏离基本面的情况（<a target="_blank" rel="noopener" href="https://academic.oup.com/qje/article-abstract/111/4/1135/1932203">$\color{blue}{Pontiff,\ 1996QJE}$</a>；<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1997.tb03807.x">$\color{blue}{Shleifer\ and\ Vishny,\ 1997JF}$</a>）。行为模型的主要预测之一是市场回报反转：当积极（消极）情绪飙升时，非理性投资者将增加（减少）资产需求，推动价格远离基本水平。行为模型预测，需求的增加（减少）将导致更高（更低）的收益，随着市场修正至其基本水平，这种回报将随着时间的推移而反转。</li></ul><h2 id="News-sentiment-embedded-in-photos-and-text"><a href="#News-sentiment-embedded-in-photos-and-text" class="headerlink" title="News sentiment embedded in photos and text"></a>News sentiment embedded in photos and text</h2><p>本节讨论主要结果：</p><ul><li>首先，证明<strong>照片中嵌入的悲观情绪预示着市场收益的反转</strong>，这与行为模型的预测一致（<a target="_blank" rel="noopener" href="https://www.journals.uchicago.edu/doi/abs/10.1086/261703">$\color{blue}{De\ Long\ et\ al.,\ 1990JPE}$</a>）；</li><li>其次，研究<strong>照片中嵌入的悲观度如何与文本中嵌入的悲观度相互作用</strong>；</li><li>第三，探讨<strong>与文字相比，哪种类型的新闻内容通过照片更有效地传播</strong>；</li><li>第四，<strong>构建了三种现实世界的交易策略，以强调分析新闻照片的益处</strong>。</li></ul><h3 id="The-impact-of-PhotoPes-on-market-returns"><a href="#The-impact-of-PhotoPes-on-market-returns" class="headerlink" title="The impact of $PhotoPes$ on market returns"></a>The impact of $PhotoPes$ on market returns</h3><p>$\color{blue}{表2}$是$PhotoPes$对滞后的市场收益进行时间序列回归的主要结果，参考 <a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock\ (2007JF)}$</a>和<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Garcia\ (2013JF)}$</a>00268-3/sbref0032)。具体而言，使用<a target="_blank" rel="noopener" href="https://www.jstor.org/stable/1913610?origin=crossref">$\color{blue}{Newey\ and\ West\ (1987Econ)}$</a>的$t$统计进行以下计算：</p><script type="math/tex;mode=display">R_{t}=\beta_{1}L5(PhotoPes_{t})+\beta_{2}L5(R_{t})+\beta_{3}L5\Big(R_{t}^{2}\Big)+\beta_{4}X_{t}+\varepsilon_{t},</script><p>其中，$R<em>{t}$表示日度对数收益&lt;CRSP价值加权（VWRETD）指数、标普500指数 (SPX)、SPDR标普500ETF (SPY)、道琼斯工业平均指数(INDU)和SPDR道琼斯工业平均指数ETF(DIA)。 $PhotoPes$ 是在$t$时刻预测为负的照片比例；$L5$将变量转换为由该变量的五个滞后组成的行向量；$X</em>{t}$是一组外生变量，包括截距、星期几指标（周一除外）以及时间$t$是否处于衰退期的指标变量。<u><strong>第$t-1$天的新闻情绪测量时间是基于第$t-1$天的新闻，这些新闻在</strong></u>$\color{red}{第t天上午发布并进入公共领域}$。除了主要股指之外，还检验了ETF，以<strong>确认结果不是由指数的非流动性成分驱动的</strong>（<a target="_blank" rel="noopener" href="https://academic.oup.com/rfs/article/28/1/1/1682440">$\color{blue}{Da\ et\ al.,\ 2015RFS}$</a>）。</p><p>$\color{blue}{表2\ Panel\ A}$表明$PhotoPes_{t-1}$<strong>与市场收益均呈负相关</strong>。</p><ul><li>经济意义：$PhotoPes$一个标准差变化对第二天的VWRETD的平均影响为$\color{red}{4.2bps}$，几乎是VWRETD无条件平均每日收益的大小（见$\color{blue}{表1\ Panel\ B}$ ，用于描述性统计）；</li><li>检验$PhotoPes$的滞后性，以确定在接下来的四个交易日内是否会出现最初跌势的反转。反转集中在二阶滞后二和五阶滞后，交易周内的$\color{red}{反转幅度在9.8至10.9个基点之间}$。</li><li>卡方检验表明，反转（$t-2$和$t-5$之间的系数之和）在INDU的1%水平以及VWRETD、SPX、SPY和DIA的5% 水平上具有统计显著性。此外，卡方检验显示，$t-1$和$t-5$之间的$PhotoPes$系数之和与零无法区分，这表明$t-1$时刻的初始下跌在交易周的剩余时间内发生反转。<strong><u>收益反转模式与悲观投资者造成的短暂价格下行压力一致。</u></strong>可以排除$PhotoPes$包含基本面信息的可能性，因为收益率最初下降之后是完全逆转。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404281516190.png" alt=""></p><p>一个重要的问题是news photos是否直接导致投资者变得更加悲观或不那么悲观？尽管<strong>不能排除新闻编辑选择照片来反映读者信念的可能性，但本文提供了一些证据表明新闻照片直接影响投资者的信念</strong>，研究了$PhotoPes$与顾问情绪报告的关系。“顾问情绪报告“调查了100多家独立投资通讯的市场观点，并将调查结果报告为看涨、看跌和预期调整的顾问的百分比。” <sup><a href="#fn_14" id="reffn_14">14</a></sup>本文对$PhotoPes$在$t-1$至$t-5$天的净看跌分数进行回归，发现$PhotoPes$预测下一个交易周的净看跌分数会增加。<sup><a href="#fn_15" id="reffn_15">15</a></sup>这项测试表明，当新闻包含许多负面图片时，财务顾问在接下来的交易周会变得更加看跌和不那么看涨。</p><p><sup><a href="#fn_14" id="reffn_14">14</a></sup>:Source: <a target="_blank" rel="noopener" href="https://www.investorsintelligence.com/x/advisors_sentiment">https://www.investorsintelligence.com/x/advisors_sentiment</a>.<br><sup><a href="#fn_15" id="reffn_15">15</a></sup>:The results are available upon request from the authors.</p><p>接下来，使用照片中负面情绪的预测可能性替换负面情绪指示变量$Neg<em>{it}$进而修改$PhotoPes$。预测可能性是分类模型对于预测$t$天的照片$i$包含负面情绪的置信度；因此，修改后的$PhotoPes$将为照片分类模型更确定的照片赋予更高的权重。在$\color{blue}{表2\ Panel\ B}$中$PhotoPes</em>{t-1}$<strong>与市场收益仍呈负相关</strong>，相较于$\color{blue}{Panel\ A}$中的基线结果，当使用修改后的$PhotoPes$时，经济规模和这种关系的统计显着性更强。注意：其余检验中仅使用未修改的$PhotoPes$。</p><h3 id="PhotoPes-and-sentiment-embedded-in-text"><a href="#PhotoPes-and-sentiment-embedded-in-text" class="headerlink" title="$PhotoPes$ and sentiment embedded in text"></a>$PhotoPes$ and sentiment embedded in text</h3><p>本节比较$PhotoPes$和$TextPes$的预测能力，以及照片和新闻文字是互补还是替代。具体而言，新闻媒体是否使用照片来enhance（complement）文本中嵌入的情感，或者是否使用照片来传达文本的alternative信息（substitutes）？鉴于新闻媒体日益多元化，这是一个需要解决的重要问题。</p><p>$\color{blue}{表3}$研究了$PhotoPes$、$TextPes$及其相互作用与市场回报的关系。运行以下回归：</p><script type="math/tex;mode=display">\begin{aligned}R_{t}&=\beta_{1}L5(PhotoPes_{t})+\beta_{2}L5(TextPes_{t})\\&+\beta_{3}  \left(PhotoPes\times TextPes\right)_{t-1}+\beta_{4}L5(R_{t})+\beta_{5}L5\left(R_{t}^{2}\right)\\&+\beta_{6}X_{t}+\varepsilon_{t},\end{aligned}</script><p>其中$TextPes_{t}$是CoreNLP模型在时间$t$（在第 2.3 节中定义）所有文章的平均悲观得分。为了更好地理解这两个变量如何相互作用，<strong>本文基于相同的文章来计算$TextPes$和$PhotoPes$（即，在$TextPes$的计算中不包括没有照片的文章）</strong>。</p><p>$\color{blue}{表3}$中控制新闻文本中的悲观情绪后，$PhotoPes<em>{t-1}$与VWRETD、SPX、SPY、INDU和DIA的市场回报呈负相关。尽管$\color{blue}{表4}$表明，**在不控制$PhotoPes$的情况下$TextPes$可以预测回报反转，但在将$PhotoPes$添加到模型中后，$\color{blue}{表3}$中$TextPe</em>{t-1}$的系数为负，但在 10% 水平上不具有统计显著性<em>*。【</em>值得注意的是，在此回归中$TextPes$不显著的一个可能原因是我们仅基于带有照片的文章计算$TextPes$。当使用视觉信息而不是文本更强烈地传达主题时，新闻编辑可能会在文章中包含照片，从而有利于我们的$PhotoPes$变量。*】</p><p>接下来，考虑$PhotoPes$和$TextPes$交互项。通过交互项$(PhotoPes\times TextPes)_{t-1}$评估新闻变量中的悲观情绪对第二天收益的补充或替代效应，并研究一个悲观变量对次日收益率的边际效应取决于另一个悲观变量的水平（<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar_lookup?title=Multiple%20Regression%3A%20Testing%20and%20Interpreting%20Interactions&amp;author=L.S.%20Aiken&amp;publication_year=1991">$\color{blue}{Aiken\ et\ al.,\ 1991Book}$</a>；<a target="_blank" rel="noopener" href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.48.7.900.2820">$\color{blue}{Siggelkow,\ 2002MS}$</a>）。</p><ul><li><p>一方面，媒体可以使用照片来强化文本中的悲观情绪。如果照片中嵌入的悲观情绪增强了文本中嵌入的悲观情绪，那么当文本中嵌入的悲观情绪较高时，照片中高水平和低水平悲观情绪之间的收益边际下降应该更高，而不是更低。为支持互补这一观点，交互项系数 $(PhotoPes\times TextPes)_{t-1}$应为负。</p></li><li><p>另一方面，媒体可以使用照片来传达文本中尚未反映的投资者情绪的另一种维度。如果文本中嵌入的悲观情绪和照片中嵌入的悲观情绪作为替代品相互作用，那么当文本中嵌入的悲观情绪较高而不是较低时，照片中嵌入的悲观情绪的高水平和低水平之间的回报边际下降应该更小。换句话说，当$TextPes$水平较高时，在照片中嵌入额外的悲观情绪不会对市场回报产生显著的边际贡献。为支持替代这一观点，交互项系数 $(PhotoPes\times TextPes)_{t-1}$应为正。</p></li><li>$\color{blue}{表3}$中系数均为正且在10%水平显著，支持替代假设<sup><a href="#fn_16" id="reffn_16">16</a></sup>。</li></ul><p><sup><a href="#fn_16" id="reffn_16">16</a></sup>:Interaction plots are available on request.</p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405051715836.png" alt=""></p><p>尽管控制$TextPes$，$PhotoPes$就占主导地位，但删除$PhotoPes$，仅$TextPes$就可以预测收益反转，从而证实了<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock\ (2007JF)}$</a>和<a target="_blank" rel="noopener" href="http://refhub.elsevier.com/S0304-405X(21">$\color{blue}{Garcia\ (2013JF)}$</a>00268-3/sbref0032)的观点。</p><ul><li>$\color{blue}{表4}$报告了市场收益（VWRETD、SPX、SPY、INDU 和 DIA）对$TextPes$和对照滞后的时间序列回归结果（不控制$PhotoPes$且$\color{red}{不要求文章包含照片}$），$TextPes_{t-1}$与SPY、INDU和DIA在5%水平以及VWRETD和SPX在10%水平的市场收益负相关；</li><li>经济意义：$TextPes$的一个标准差变化对第二天INDU的平均影响为8.5bps，几乎是INDU无条件平均日收益率的2.2 倍。这一经济规模与<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x">$\color{blue}{Tetlock\ (2007JF)}$</a>的报告一致（8.1bps）；</li><li>此外，SPY和INDU的$t-2$系数仍然为负且显著，这表明<strong><u>与照片相比，市场需要更多时间反映新闻文本中的信息</u></strong>。$TextPes$的反转集中在$t-5$，而对于大多数规范，基线结果中$PhotoPes$的反转较早在$t-2$开始。对于INDU和DIA，$t-2$和$t-5$之间的反转幅度在8.0到8.1bps之间，并且在10%水平上具有统计显著性。考虑到规范1至3中$TextPes$的$t-2$和$t-5$之间缺乏显著反转，但所有规范中$t-5$处的显著正系数，得出结论：<strong><u>初始效果仅部分反转</u></strong>。</li><li>总体而言，这一结果与<u><strong>$TextPes$同时包含情绪和基本面信息相一致</strong></u>；因此，它与市场回报的关系只是$\color{red}{部分过渡性}$的。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405051900918.png" alt=""></p><h3 id="Attention-and-PhotoPes"><a href="#Attention-and-PhotoPes" class="headerlink" title="Attention and $PhotoPes$"></a>Attention and $PhotoPes$</h3><p><strong>照片是否可以在报纸上发挥吸引注意力的作用？</strong></p><p>唤起观众强烈情绪的照片可能会分散新闻文本的注意力，进而主导悲观-回归关系。研究表明，在新闻中加入照片可能会引人注目。例如，<a target="_blank" rel="noopener" href="https://www.biblio.com/book/eyes-news-pegie-starkmario-r-garcia/d/1435923367">$\color{blue}{Garcia\ and\ Stark\ (1991Book)}$</a>进行了一项眼球追踪研究，并记录了照片如何成为报纸页面最常见的最初吸引力。<a target="_blank" rel="noopener" href="https://academic.oup.com/joc/article-abstract/65/6/997/4082343">$\color{blue}{Powell\ et\ al.\ (2015JoC)}$</a>发现了类似的证据，表明带有照片的文本内容或仅包含照片的内容比仅包含文本的内容更引人注目。在这些研究的推动下，本文检验了新闻中蕴含的悲观情绪与市场回报之间的关系如何根据显著新闻照片的存在而变化。</p><p>为检验新闻照片的吸引注意力的作用，运行以下回归区分$PhotoPes$和$TextPes$在照片突出期间对市场回报的影响：</p><script type="math/tex;mode=display">\begin{aligned}
R_{t}&=(E_{t})[\beta_{1}L5(PhotoPes_{t})+\beta_{2}L5(TextPes_{t})+\beta_3(PhotoPes\times TextPes)_{t-1}\\&+\beta_4L5(R_t)+\beta_5L5\Big(R_t^2\Big)\Big]+(1-E_t)[\gamma_1L5(PhotoPes_t)+\gamma_2L5(TextPes_t)\\&+\gamma_3(PhotoPes\times TextPes)_{t-1}+\gamma_4L5(R_t)+\gamma_5L5\Big(R_t^2\Big)\Big]+\beta_{6}X_{t}+\varepsilon_{t},
\end{aligned}</script><p>其中$E<em>{t}$是一个指示变量，如果$t$日位于$PhotoPes$的顶部或底部十分位数，则该变量的值为1。作者假设，当大多数照片具有相同的情绪（即照片绝大多数是正面或负面）时，$E</em>{t}=1$，读者会发现照片中的一致信息很突出，因此会关注照片而不是文本。另一方面，当照片具有中性或混合情绪时，$E_{t}=0$，读者会发现照片中的混合信息较弱，因此会关注文本。</p><p>$\color{blue}{表5}$为假设提供支撑：</p><ul><li>在5%水平上，$PhotoPes$在照片突出的时期与第二天的市场收益率负相关；</li><li>此外，在照片突出的时期，$TextPes$与第二天的市场收益在统计上没有相关性。</li><li>相比之下，在照片不显著的时期，$TextPes$与第二天的市场收益呈负相关，SPX、SPY、INDU和DIA的水平为5%，VWRETD的水平为10%。</li><li>此外，在照片不显著的时期，$PhotoPes$与第二天的市场回报没有显著相关。</li></ul><p>总体而言，$\color{blue}{表5}$表明，在照片突出的日子里，$PhotoPes$占主导地位，而$TextPes$并不重要。相比之下，在照片不显著的日子里，$TextPes$占主导地位，而 $PhotoPes$则没有统计意义。照片可能会吸引整篇文章（包括文字）的注意力；然而，实证结果并不支持这种可能性，因为在照片显著（不显著）的时期，$TextPes_{t-1}$的系数是微不显著（显著）<sup><a href="#fn_17" id="reffn_17">17</a></sup>。</p><blockquote id="fn_17"><sup>17</sup>. 在未展示的结果中，记录了表5中的显著性效应是不对称的。在大多数照片都是正面的日子里，这种效果最强（最低的照片十分位数）。这一证据与Sicherman et al.（2015）一致，他们发现投资者具有选择性关注，在市场下跌期间，投资者对金融市场的关注度暴跌9.5%。当$PhotoPes$较高（负面照片较多）时，投资者对照片的关注较少，因此与$PhotoPes$较低的日子相比，效果较小。<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405052152332.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405052153915.png" alt=""></p><h3 id="Which-information-is-more-effectively-transmitted-by-photos"><a href="#Which-information-is-more-effectively-transmitted-by-photos" class="headerlink" title="Which information is more effectively transmitted by photos?"></a>Which information is more effectively transmitted by photos?</h3><p><strong>本文试图回答新闻照片捕捉到哪些文字无法捕捉到的信息。</strong></p><p>先前的研究表明，照片可以成为捕捉创伤事件的更有效的媒介（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1023/A:1024728626505">$\color{blue}{Chemtob\ et\ al.,\ 1999JTS}$</a>）。作者检验了在恐惧加剧期间，市场收益与照片和文本中嵌入的悲观情绪之间的关系是否会发生变化。</p><p>使用TRMI（汤森路透市场心理指数）指标衡量恐惧加剧的时期，这些指标从广泛的新闻媒体来源和社交媒体内容中捕捉不同主题和情绪的基调。TRMI使用专有字典对$t$天内不同情绪和事件的语气进行分类，并以0到1的等级进行量化；分数越高，事件或情绪越普遍。为检验使用文本和照片的新闻中的悲观情绪与恐惧水平的市场回报之间的关系，采用以下回归：</p><script type="math/tex;mode=display">\begin{aligned}R_{t}&=(F_{t})[\beta_{1}L5(PhotoPes_{t})+\beta_{2}L5(TextPes_{t})+\beta_{3}(PhotoPes\times TextPes)_{t-1}\\&+\beta_{4}L5(R_{t})+\beta_{5}L5(R_{t}^{2})]+(1-F_{t})[\gamma_{1}L5(PhotoPes_{t})+\gamma_{2}L5(TextPes_{t})\\&+\gamma_{3}(PhotoPes\times TextPes)_{t-1}+\gamma_{4}L5(R_{t})+\gamma_{5}L5(R_{t}^{2})]+\beta_{6}X_{t}+\varepsilon_{t},\end{aligned}</script><p>其中$F_{t}$是指示变量，如果$t$天的恐惧分数高于中位数（计算为以下主题的平均TRMI分数：fear and gloom），则该指标变量的值为 1。</p><p>$\color{blue}{表6}$五个规范中，无论恐惧程度高还是低，$PhotoPes<em>{t-1}$都与市场回报呈负相关。然而，在高度恐惧期间，幅度要大得多：例如，在高度恐惧期间，$PhotoPes$的一个标准差变化对第二天VWRETD的平均影响为10.3 bps，而在低度恐惧期间仅为3.7 bps。但对于$TextPes$却并非如此。在所有规范中，$TextPes</em>{t-1}$均为负值但并不显著，且在高或低恐惧水平期间，系数的大小相似<sup><a href="#fn_18" id="reffn_18">18</a></sup>。</p><blockquote id="fn_18"><sup>18</sup>. 好奇的读者可能会担心，$F_{t}$变量只是挑选出了$PhotoPes$较高的日子。这并不值得担心，因为TRMI的恐惧和忧郁得分与照片指数之间的相关性很低，而且不显著（相关性为-0.025，P值=0.16）。<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405060057416.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405060057296.png" alt=""></p><p>总体而言，在恐惧程度较高的时期，照片中嵌入的悲观系数大约是恐惧程度较低时期的2.8倍，而$TextPes$的系数在这两个时期相似。这一证据表明，在传达恐惧或战争等创伤性事件方面，照片比文字更有效（<a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1023/A:1024728626505">$\color{blue}{Chemtob\ et\ al.,\ 1999JTS}$</a>）。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Hirshleifer, D., &amp; Shumway, T. (2003). Good day sunshine: Stock returns and the weather. <em>The Journal of Finance</em>, <em>58</em>(3), 1009-1032.</li><li>Edmans, A., Garcia, D., &amp; Norli, Ø. (2007). Sports sentiment and stock returns. <em>The Journal of Finance</em>, <em>62</em>(4), 1967-1998.</li><li>Tetlock, P. C. (2007). Giving content to investor sentiment: The role of media in the stock market. <em>The Journal of Finance</em>, <em>62</em>(3), 1139-1168.</li><li>Spiegel, M. (2008). Forecasting the equity premium: Where we stand today. <em>The Review of Financial Studies</em>, <em>21</em>(4), 1453-1454.</li><li>Baker, M., &amp; Wurgler, J. (2006). Investor sentiment and the cross‐section of stock returns. <em>The Journal of Finance</em>, <em>61</em>(4), 1645-1680.</li><li>Kozak, S., Nagel, S., &amp; Santosh, S. (2018). Interpreting factor models. <em>The Journal of Finance</em>, <em>73</em>(3), 1183-1223.</li><li>Chemtob, C. M., Roitblat, H. L., Hamada, R. S., Muraoka, M. Y., Carlson, J. G., &amp; Bauer, G. B. (1999). Compelled attention: The effects of viewing trauma‐related stimuli on concurrent task performance in posttraumatic stress disorder. <em>Journal of Traumatic Stress: Official Publication of The International Society for Traumatic Stress Studies</em>, <em>12</em>(2), 309-326.</li><li>Vissing-Jorgensen, A. (2003). Perspectives on behavioral finance: Does” irrationality” disappear with wealth? Evidence from expectations and actions. <em>NBER macroeconomics annual</em>, <em>18</em>, 139-194.</li><li>Singer, E. (2002). The use of incentives to reduce nonresponse in household surveys. <em>Survey nonresponse</em>, <em>51</em>(1), 163-177.</li><li>Mullainathan, S., &amp; Spiess, J. (2017). Machine learning: an applied econometric approach. <em>Journal of Economic Perspectives</em>, <em>31</em>(2), 87-106.</li><li>Manela, A., &amp; Moreira, A. (2017). News implied volatility and disaster concerns. <em>Journal of Financial Economics</em>, <em>123</em>(1), 137-162.</li><li>Buehlmaier, M. M., &amp; Whited, T. M. (2018). Are financial constraints priced? Evidence from textual analysis. <em>The Review of Financial Studies</em>, <em>31</em>(7), 2693-2728.</li><li>Gu, S., Kelly, B., &amp; Xiu, D. (2020). Empirical asset pricing via machine learning. <em>The Review of Financial Studies</em>, <em>33</em>(5), 2223-2273.</li><li>Feng, G., Giglio, S., &amp; Xiu, D. (2020). Taming the factor zoo: A test of new factors. <em>The Journal of Finance</em>, <em>75</em>(3), 1327-1370.</li><li>Shiller, R. J. (2015). Irrational exuberance: Revised and expanded third edition. In <em>Irrational exuberance</em>. Princeton university press.</li><li>Garcia, D. (2013). Sentiment during recessions. <em>The Journal of Finance</em>, <em>68</em>(3), 1267-1300.</li><li>De Long, J. B., Shleifer, A., Summers, L. H., &amp; Waldmann, R. J. (1990). Noise trader risk in financial markets. <em>Journal of Political Economy</em>, <em>98</em>(4), 703-738.</li><li>Pontiff, J. (1996). Costly arbitrage: Evidence from closed-end funds. <em>The Quarterly Journal of Economics</em>, <em>111</em>(4), 1135-1151.</li><li>Shleifer, A., &amp; Vishny, R. W. (1997). The limits of arbitrage. <em>The Journal of Finance</em>, <em>52</em>(1), 35-55.</li><li>Chemtob, C. M., Roitblat, H. L., Hamada, R. S., Muraoka, M. Y., Carlson, J. G., &amp; Bauer, G. B. (1999). Compelled attention: The effects of viewing trauma‐related stimuli on concurrent task performance in posttraumatic stress disorder. <em>Journal of Traumatic Stress: Official Publication of The International Society for Traumatic Stress Studies</em>, <em>12</em>(2), 309-326.</li><li>Zhou, G. (2018). Measuring investor sentiment. <em>Annual Review of Financial Economics</em>, <em>10</em>, 239-259.</li><li>Da, Z., Engelberg, J., &amp; Gao, P. (2015). The sum of all FEARS investor sentiment and asset prices. <em>The Review of Financial Studies</em>, <em>28</em>(1), 1-32.</li><li>Chen, H., De, P., Hu, Y., &amp; Hwang, B. H. (2014). Wisdom of crowds: The value of stock opinions transmitted through social media. <em>The review of financial studies</em>, <em>27</em>(5), 1367-1403.</li><li>Loughran, T., &amp; McDonald, B. (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks. <em>The Journal of Finance</em>, <em>66</em>(1), 35-65.</li><li>Jiang, F., Lee, J., Martin, X., &amp; Zhou, G. (2019). Manager sentiment and stock returns. <em>Journal of Financial Economics</em>, <em>132</em>(1), 126-149.</li><li>Hirshleifer, D., &amp; Shumway, T. (2003). Good day sunshine: Stock returns and the weather. <em>The Journal of Finance</em>, <em>58</em>(3), 1009-1032.</li><li>Hirshleifer, D. (2001). Investor psychology and asset pricing. <em>The journal of Finance</em>, <em>56</em>(4), 1533-1597.</li><li>Shiller, R. J. (2015). Irrational exuberance: Revised and expanded third edition. In <em>Irrational exuberance</em>. Princeton university press.</li><li>Mullainathan, S., &amp; Shleifer, A. (2005). The market for news. <em>American Economic Review</em>, <em>95</em>(4), 1031-1053.</li><li>Gentzkow, M., &amp; Shapiro, J. M. (2010). What drives media slant? Evidence from US daily newspapers. <em>Econometrica</em>, <em>78</em>(1), 35-71.</li><li>Nelson, D. L., Reed, V. S., &amp; Walling, J. R. (1976). Pictorial superiority effect. <em>Journal of Experimental Psychology: Human Learning and Memory</em>, <em>2</em>(5), 523.</li><li>Paivio, A. (1991). <em>Images in mind: the evolution of a theory</em>. Harvester Wheatsheaf.</li><li>Newhagen, J. E., &amp; Reeves, B. (1992). The evening’s bad news: Effects of compelling negative television news images on memory. <em>Journal of Communication</em>, <em>42</em>(2), 25-41.</li><li>Garcia, M., Stark, P. (1991). Eyes On the News. Poynter Institute for Media Studies, St. Petersburg, FL.</li><li>Powell, T. E., Boomgaarden, H. G., De Swert, K., &amp; De Vreese, C. H. (2015). A clearer picture: The contribution of visuals and text to framing effects. <em>Journal of Communication</em>, <em>65</em>(6), 997-1017.</li><li>Chaiken, S., &amp; Eagly, A. H. (1976). Communication modality as a determinant of message persuasiveness and message comprehensibility. <em>Journal of Personality and Social Psychology</em>, <em>34</em>(4), 605.</li><li>Todorov, A., Mandisodza, A. N., Goren, A., &amp; Hall, C. C. (2005). Inferences of competence from faces predict election outcomes. <em>Science</em>, <em>308</em>(5728), 1623-1626.</li><li>Duarte, J., Siegel, S., &amp; Young, L. (2012). Trust and credit: The role of appearance in peer-to-peer lending. <em>The Review of Financial Studies</em>, <em>25</em>(8), 2455-2484.</li><li>Halford, J. T., &amp; Hsu, H. C. S. (2020). Beauty is wealth: CEO attractiveness and firm value. <em>Financial Review</em>, <em>55</em>(4), 529-556.</li><li>Graham, J. R., Harvey, C. R., &amp; Puri, M. (2017). A corporate beauty contest. <em>Management Science</em>, <em>63</em>(9), 3044-3056.</li><li>Bazley, W. J., Cronqvist, H., &amp; Mormann, M. (2021). Visual finance: The pervasive effects of red on investor behavior. <em>Management science</em>, <em>67</em>(9), 5616-5641.</li><li>Blankespoor, E., Hendricks, B. E., &amp; Miller, G. S. (2017). Perceptions and price: Evidence from CEO presentations at IPO roadshows. <em>Journal of Accounting Research</em>, <em>55</em>(2), 275-327.</li><li>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>25</em>.</li><li>Yu, J., Wang, Z., Majumdar, A., &amp; Rajagopal, R. (2018). DeepSolar: A machine learning framework to efficiently construct a solar deployment database in the United States. <em>Joule</em>, <em>2</em>(12), 2605-2617.</li><li>Scoles, S. , 2019. Researchers spy signs of slavery from space. <em>Science</em> 363(6429), 804.</li><li>Jean, N., Burke, M., Xie, M., Davis, W. M., Lobell, D. B., &amp; Ermon, S. (2016). Combining satellite imagery and machine learning to predict poverty. <em>Science</em>, <em>353</em>(6301), 790-794.</li><li>You, Q., Luo, J., Jin, H., &amp; Yang, J. (2015, February). Robust image sentiment analysis using progressively trained and domain transferred deep networks. In <em>Proceedings of the AAAI conference on Artificial Intelligence</em> (Vol. 29, No. 1).</li><li>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2818-2826).</li><li>Yang, L., Hanneke, S., &amp; Carbonell, J. (2013). A theory of transfer learning with applications to active learning. <em>Machine Learning</em>, <em>90</em>, 161-189.</li><li>Campos, V., Jou, B., &amp; Giro-i-Nieto, X. (2017). From pixels to sentiment: Fine-tuning CNNs for visual sentiment prediction. <em>Image and Vision Computing</em>, <em>65</em>, 15-22.</li><li>Cong, L. W., Liang, T., &amp; Zhang, X. (2019). Textual factors: A scalable, interpretable, and data-driven approach to analyzing unstructured information. <em>Interpretable, and Data-driven Approach to Analyzing Unstructured Information (September 1, 2019)</em>.</li><li>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., &amp; Potts, C. (2013, October). Recursive deep models for semantic compositionality over a sentiment treebank. In <em>Proceedings of the 2013 conference on empirical methods in natural language processing</em> (pp. 1631-1642).</li><li>Tversky, A., &amp; Kahneman, D. (1983). Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. <em>Psychological Review</em>, <em>90</em>(4), 293.</li><li>Fischhoff, B., Slovic, P., &amp; Lichtenstein, S. (1977). Knowing with certainty: The appropriateness of extreme confidence. <em>Journal of Experimental Psychology: Human perception and performance</em>, <em>3</em>(4), 552.</li><li>Newey, W.K., West, K.D., 1987. A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. <em>Econometrica</em> 55, 703-708.</li><li>Aiken, L. S., West, S. G., &amp; Reno, R. R. (1991). Multiple regression: Testing and interpreting interactions. sage.</li><li>Siggelkow, N. (2002). Misperceiving interactions among complements and substitutes: Organizational consequences. <em>Management science</em>, 48(7), 900-916.</li><li>Sicherman, N., Loewenstein, G., Seppi, D. J., &amp; Utkus, S. P. (2016). Financial attention. <em>The Review of Financial Studies</em>, 29(4), 863-897.</li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://zhichonglyu.com">Zhichong Lyu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://zhichonglyu.com/49582f09/">https://zhichonglyu.com/49582f09/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/JFE/">JFE</a><a class="post-meta__tags" href="/tags/Asset-pricing/">Asset pricing</a><a class="post-meta__tags" href="/tags/Multimodal/">Multimodal</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230036336.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>Sponsor</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202402010533914.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202402010533914.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202402010533363.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202402010533363.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/4306b409/" title="Journal | JFuM | Anger in predicting the index futures returns"><img class="cover" src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230009710.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Journal | JFuM | Anger in predicting the index futures returns</div></div></a></div><div class="next-post pull-right"><a href="/5886b203/" title="Journal | JBF | How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk"><img class="cover" src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202405130108507.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">Journal | JBF | How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/c86b3943/" title="Journal | JFE | When can the market identify old news?"><img class="cover" src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202403172015901.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-18</div><div class="title">Journal | JFE | When can the market identify old news?</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404291749220.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">Zhichong Lyu</div><div class="author-info__description">Southwest Jiaotong University<br>四川万悟昇长科技有限公司<br>https://www.wanwuera.com/</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://space.bilibili.com/3546619203029026"><i class="fab fa-bilibili"></i><span>Follow</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://scholar.google.com/citations?user=yI2F_VEAAAAJ&amp;hl" target="_blank" title="Scholar"><i class="fab fa-brands fa-google-scholar" style="color:#4a7dbe"></i></a><a class="social-icon" href="https://github.com/lyuzhichong" target="_blank" title="Github"><i class="fab fa-github" style="color:#4a7dbe"></i></a><a class="social-icon" href="mailto:lyuzhichong@my.swjtu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a><a class="social-icon" href="http://wpa.qq.com/msgrd?v=3&amp;uin=2278455128&amp;site=qq&amp;menu=yes" target="_blank" title="QQ"><i class="fab fa-qq" style="color:#4a7dbe"></i></a><a class="social-icon" href="https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202410021805008.jpg" target="_blank" title="WeChat"><i class="fas fa-brands fa-weixin" style="color:#4a7dbe"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%95%E8%B5%84%E8%80%85%E6%83%85%E7%BB%AA"><span class="toc-number">1.1.</span> <span class="toc-text">投资者情绪</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">1.2.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data"><span class="toc-number">1.4.</span> <span class="toc-text">Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Finding"><span class="toc-number">1.5.</span> <span class="toc-text">Finding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E7%8C%AE%E6%89%A9%E5%B1%95"><span class="toc-number">1.6.</span> <span class="toc-text">文献扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%8A%95%E8%B5%84%E8%80%85%E6%83%85%E7%BB%AA%E6%96%87%E7%8C%AE%E7%9B%B8%E5%85%B3"><span class="toc-number">1.6.1.</span> <span class="toc-text">与投资者情绪文献相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E6%8A%95%E8%B5%84%E8%80%85%E6%83%85%E7%BB%AA%E5%92%8C%E6%96%B0%E9%97%BB%E6%96%87%E7%8C%AE"><span class="toc-number">1.6.2.</span> <span class="toc-text">扩展投资者情绪和新闻文献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E8%A7%86%E8%A7%89%E5%88%BA%E6%BF%80%E5%BF%83%E7%90%86%E5%AD%A6%E6%96%87%E7%8C%AE%E6%89%A9%E5%B1%95%E5%88%B0%E9%87%91%E8%9E%8D%E5%92%8C%E7%BB%8F%E6%B5%8E%E5%AD%A6"><span class="toc-number">1.6.3.</span> <span class="toc-text">将视觉刺激心理学文献扩展到金融和经济学</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E5%85%B3%E4%BA%8E%E8%A7%86%E8%A7%89%E5%86%85%E5%AE%B9%E9%A2%84%E6%B5%8B%E9%87%91%E8%9E%8D%E5%B8%82%E5%9C%BA"><span class="toc-number">1.6.4.</span> <span class="toc-text">扩展关于视觉内容预测金融市场</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-1"><span class="toc-number">2.</span> <span class="toc-text">Data</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Photo-classification"><span class="toc-number">2.1.</span> <span class="toc-text">Photo classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%85%A7%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BB%BB%E5%8A%A1%E6%98%AF%E8%83%BD%E5%A4%9F%E5%9C%A8%E6%9C%80%E5%B0%91%E4%BA%BA%E5%B7%A5%E5%8F%82%E4%B8%8E%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E8%AF%86%E5%88%AB%E7%85%A7%E7%89%87%E5%86%85%E5%AE%B9"><span class="toc-number">2.1.1.</span> <span class="toc-text">机器学习照片分类模型的主要任务是能够在最少人工参与的情况下识别照片内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%83%BD%E5%A4%9F%E9%A2%84%E6%B5%8B%E7%85%A7%E7%89%87%E5%8F%AF%E8%83%BD%E5%BC%95%E5%8F%91%E6%8A%95%E8%B5%84%E8%80%85%E6%83%85%E7%BB%AA%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">构建能够预测照片可能引发投资者情绪的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AA%E5%BC%80%E5%8F%91%E6%9B%B4%E7%B2%BE%E7%BB%86%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E5%9B%A0"><span class="toc-number">2.1.3.</span> <span class="toc-text">未开发更精细分类模型原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.4.</span> <span class="toc-text">迁移学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%A3%80%E9%AA%8C%EF%BC%88%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%89"><span class="toc-number">2.1.5.</span> <span class="toc-text">模型检验（测试集）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%A3%80%E9%AA%8C%EF%BC%88%E4%BA%BA%E5%B7%A5%E6%A0%87%E6%B3%A8WSJ%E6%95%B0%E6%8D%AE%EF%BC%89"><span class="toc-number">2.1.6.</span> <span class="toc-text">模型检验（人工标注WSJ数据）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Wall-Street-Journal-sample"><span class="toc-number">2.2.</span> <span class="toc-text">The Wall Street Journal sample</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Variable-construction"><span class="toc-number">2.3.</span> <span class="toc-text">Variable construction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PhotoPes"><span class="toc-number">2.3.1.</span> <span class="toc-text">$PhotoPes$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TextPes"><span class="toc-number">2.3.2.</span> <span class="toc-text">$TextPes$</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Descriptive-statistics"><span class="toc-number">2.4.</span> <span class="toc-text">Descriptive statistics</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Results"><span class="toc-number">3.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#News-sentiment-embedded-in-photos-and-text"><span class="toc-number">3.1.</span> <span class="toc-text">News sentiment embedded in photos and text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-impact-of-PhotoPes-on-market-returns"><span class="toc-number">3.1.1.</span> <span class="toc-text">The impact of $PhotoPes$ on market returns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PhotoPes-and-sentiment-embedded-in-text"><span class="toc-number">3.1.2.</span> <span class="toc-text">$PhotoPes$ and sentiment embedded in text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-and-PhotoPes"><span class="toc-number">3.1.3.</span> <span class="toc-text">Attention and $PhotoPes$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-information-is-more-effectively-transmitted-by-photos"><span class="toc-number">3.1.4.</span> <span class="toc-text">Which information is more effectively transmitted by photos?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#References"><span class="toc-number">4.</span> <span class="toc-text">References</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/a0b08951/" title="Draft | DeepSeek-R1">Draft | DeepSeek-R1</a><time datetime="2025-02-03T00:00:00.000Z" title="Created 2025-02-03 08:00:00">2025-02-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/5b412a65/" title="Journal | MS | ChatGPT for textual analysis? How to use generative LLMs in accounting research">Journal | MS | ChatGPT for textual analysis? How to use generative LLMs in accounting research</a><time datetime="2025-01-18T00:00:00.000Z" title="Created 2025-01-18 08:00:00">2025-01-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/344f9fba/" title="Journal | RSER | Study on how the digital economy affects urban carbon emissions">Journal | RSER | Study on how the digital economy affects urban carbon emissions</a><time datetime="2024-10-01T00:00:00.000Z" title="Created 2024-10-01 08:00:00">2024-10-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/abb09484/" title="[object Object]">[object Object]</a><time datetime="2024-09-18T00:00:00.000Z" title="Created 2024-09-18 08:00:00">2024-09-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/5886b203/" title="Journal | JBF | How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk">Journal | JBF | How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk</a><time datetime="2024-05-13T00:00:00.000Z" title="Created 2024-05-13 08:00:00">2024-05-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/49582f09/" title="[object Object]">[object Object]</a><time datetime="2024-05-06T00:00:00.000Z" title="Created 2024-05-06 08:00:00">2024-05-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://cdn.jsdelivr.net/gh/zhichonglyu/blogimg/img/202404230036336.png)"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By Zhichong Lyu</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const n of document.querySelectorAll('script[type^="math/tex"]')){var e=!!n.type.match(/; *mode=display/),e=new t.options.MathItem(n.textContent,t.inputJax[0],e),a=document.createTextNode("");n.parentNode.replaceChild(a,n),e.start={node:a,delim:"",n:0},e.end={node:a,delim:"",n:0},t.math.push(e)}},""]}}};const a=document.createElement("script");a.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",a.id="MathJax-script",a.async=!0,document.head.appendChild(a)}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div",{class:"katex-wrap"})})</script><script>(()=>{const e=()=>{new Valine(Object.assign({el:"#vcomment",appId:"DN6pZYKnpWIhedq7KvFKkfBE-gzGzoHsz",appKey:"jhj1ilYegANc4yLgz9y9T9W9",avatar:"robohash",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))};var a=async()=>{"function"==typeof Valine||await getScript("https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js"),e()};setTimeout(a,0)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3.5.2/dist/style.min.css"><script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3.5.2/dist/umd/index.min.js"></script><script>(()=>{docsearch(Object.assign({appId:"W01O36M4KE",apiKey:"915b5455ecf60c1382d92945266a4c19",indexName:"hexo",container:"#docsearch"},null));const e=()=>{document.querySelector(".DocSearch-Button").click()};var c=()=>{btf.addEventListenerPjax(document.querySelector("#search-button > .search"),"click",e)};c(),window.addEventListener("pjax:complete",c)})()</script></div></div></body></html>